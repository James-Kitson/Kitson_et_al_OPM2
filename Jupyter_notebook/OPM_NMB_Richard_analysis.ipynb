{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Jupyter notebook for processing analysis run 2 as presented in Kitson _et al._ 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Downloading raw read data from SRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell will download the raw Illumina data associated with the study and rename the files as specified in the text file `SraAccList.txt` provided in the `data` directory.\n",
    "\n",
    "The cell expects modules of the [SRA Toolkit](http://www.ncbi.nlm.nih.gov/Traces/sra/?view=toolkit_doc) (`prefetch` and `fastq-dump`) to be in your path (tested with SRA Toolkit version 2.3.5). `prefetch` downloads data in sra format and places it per default into `~/ncbi/public/sra/`. `fastq-dump` converts the sra formatted data to gzipped fastq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "sra_path=\"~/ncbi/public/sra\"\n",
    "\n",
    "cd data\n",
    "\n",
    "for line in $(cat SraAccList_run2.txt)\n",
    "do \n",
    "    name=$(echo -e \"$line\" | cut -d \",\" -f 1)\n",
    "    acc=$(echo -e \"$line\" | cut -d \",\" -f 2)\n",
    "    \n",
    "    prefetch $acc\n",
    "    fastq-dump --split-files --gzip --defline-seq '@$ac-$sn/$ri' --defline-qual '+' $sra_path/$acc.sra\n",
    "    mv $acc\\_1.fastq.gz $name\\_1.fastq.gz\n",
    "    mv $acc\\_2.fastq.gz $name\\_2.fastq.gz\n",
    "done\n",
    "\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Processing read data\n",
    "To ensure complete reproducibility all subsequent processing of the read data was performed in a docker container which we have made available [here](https://hub.docker.com/r/chrishah/metabeat/).\n",
    "\n",
    "The following cell will print out the help file for metaBEAT_global.py allowing readers to follow the commands used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "metaBEAT_global.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###PERFORM A CLUSTERING ANALYSIS TO CHECK FOR OPTIMAL CLUSTER READ DEPTH\n",
    "We will ultimately be clustering and filtering our data to unique sequences but we don't know what read depth we should accept as a minimum valid cluster size. To investigate this we will iterate across different minimum read depths and examine the effect this has on clusters retained for taxonomic assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Move to the correct folder\n",
    "mkdir -p ../Processed_data/Cluster_size\n",
    "cd ../Processed_data/Cluster_size\n",
    "\n",
    "# Loop across minimum cluster sizes\n",
    "for i in $(seq 71 5 101)\n",
    "do\n",
    "      echo -e \"running with coverage $i\"\n",
    "      metaBEAT_global.py -Q ../../data/QueryMap_all.txt --cluster --merge --merged_only \\\n",
    "      --length_filter 310 --product_length 400 --clust_match 1.0 --clust_cov $i \\\n",
    "      --trim_minlength 100 -@ James.Kitson@newcastle.ac.uk -o metaBEAT &> cluster_log$i.txt\n",
    "      mv metaBEAT_read_stats.csv metaBEAT_read_stats_$i.csv\n",
    "done\n",
    "\n",
    "# Combine the files into one large output for plotting in R\n",
    "cat metaBEAT_read_stats_6.csv | head -n 1 > ../../data/Richard_combined_read_stats_all.csv\n",
    "cat metaBEAT_read_stats_* | grep \"sample,\" -v >> ../../data/Richard_combined_read_stats_all.csv\n",
    "cd ..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###PERFORM A MISTAGGING ANALYSIS TO CHECK FOR BACKGROUND NOISE READ DEPTH\n",
    "We also need to check the illegal tag combinations to make sure that mistagging is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Move to the correct folder\n",
    "mkdir -p ../Processed_data/Cluster_size/run2_mistag\n",
    "cd ../Processed_data/Cluster_size/run2_mistag\n",
    "\n",
    "# Loop across minimum cluster sizes\n",
    "for i in $(seq 6 5 101)\n",
    "do\n",
    "      echo -e \"running with coverage $i\"\n",
    "      metaBEAT_global.py -Q ../../../data/Illegal_QueryMap_run2.txt --cluster --merge --merged_only \\\n",
    "      --length_filter 310 --product_length 400 --clust_match 1.0 --clust_cov $i \\\n",
    "      --trim_minlength 100 -@ James.Kitson@newcastle.ac.uk -o metaBEAT &> cluster_log_$i.txt\n",
    "      mv metaBEAT_read_stats.csv metaBEAT_read_stats_$i.csv\n",
    "done\n",
    "\n",
    "# Combine the files into one large output for plotting in R\n",
    "cat metaBEAT_read_stats_6.csv | head -n 1 > ../../../data/combined_illegal_read_stats_run2.csv\n",
    "cat metaBEAT_read_stats_* | grep \"sample,\" -v >> ../../../data/combined_illegal_read_stats_run2.csv\n",
    "cd ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###PERFORMING FINAL ANALYSES (INCL. TAXONOMIC ASSIGNMENT) FROM RAW READ DATA\n",
    "####From the clustering analysis we have two possible thresholds for minimum cluster size that we could use:\n",
    "\n",
    "* Minimum cluster size for clean negatives - __> ~36 reads__.\n",
    "* Minimum cluster size for stable read depths across wells (i.e. removal of rare sequences) - __> ~51 reads__.\n",
    "\n",
    "####From the mistagging analysis we have an additional minimum cluster size we can use:\n",
    "\n",
    "* Minimum cluster size for mistagging removal - __> ~66 reads__.\n",
    "\n",
    "####We will now iterate across these values and compare sequencing success in the [Rnotebook](https://github.com/HullUni-bioinformatics/Kitson_et_al_NMB/blob/master/R_plotting_notebook_run2.Rmd) to examine what effect this has on the ecological parameters we are interested in:\n",
    " \n",
    "1. Parasitoid identity.\n",
    "2. Percentage parasitism for each Parasitoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Move to the correct folder\n",
    "mkdir -p ../Processed_data/Final_analysis\n",
    "cd ../Processed_data/Final_analysis\n",
    "\n",
    "metaBEAT_global.py -Q ../../data/QueryMap_all.txt --cluster --merge --merged_only \\\n",
    "      --length_filter 310 --product_length 400 --clust_match 1.0 --clust_cov 50 --trim_minlength 100 -E \\\n",
    "      -n 6 -v --blast --blast_db ../../../../Genbank/nt/nt \\\n",
    "      --min_ident 0.95 --min_ali_length 0.90 -@ James.Kitson@newcastle.ac.uk &> OPM_Richard_BLAST_log.txt\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MetaBEAT produces files in the .biom format for easy use with QIIME but we will be using the output in R for plotting figures. The next cell strips out all the QIIME .biom formatting and leaves us with a rectangular read table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: ./GLOBAL/BLAST_0.95/metaBEAT-by-taxonomy-readcounts.blast.tsv: No such file or directory\n",
      "bash: line 4: ./GLOBAL/BLAST_0.95/metaBEAT-processed.tsv: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../Processed_data/Final_analysis/\n",
    "\n",
    "cat ./GLOBAL/BLAST_0.95/metaBEAT-by-taxonomy-readcounts.blast.tsv | \\\n",
    "grep \"# \" -v | sed 's/#//' | sed 's/\\.blast//g' | sed 's/ /_/' | perl -ne \\\n",
    "'chomp; @a=split(\"\\t\"); pop(@a); $out=join(\"\\t\", @a); print \"$out\\n\"' \\\n",
    "> ./GLOBAL/BLAST_0.95/metaBEAT-processed.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reformatted metaBEAT data could be used as is but we'll transpose our table now to make life easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ../Processed_data/Final_analysis/GLOBAL/BLAST_0.95/metaBEAT-processed.tsv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-de1788fc03ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moutput_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../data/Richard_metaBEAT_transpose.tsv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[0;32m    463\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1054\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3129)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:5760)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File ../Processed_data/Final_analysis/GLOBAL/BLAST_0.95/metaBEAT-processed.tsv does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "filename = \"../Processed_data/Final_analysis/GLOBAL/BLAST_0.95/metaBEAT-processed.tsv\"\n",
    "output_filename = \"../data/Richard_metaBEAT_transpose.tsv\"\n",
    "\n",
    "df = pd.read_table(filename)\n",
    "df = df.transpose()\n",
    "df.to_csv(output_filename, header=None, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
